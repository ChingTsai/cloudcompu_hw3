import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.fs.Path
import org.apache.spark.HashPartitioner


object InvertedIdx {
  def main(args: Array[String]) {

    val filePath = args(0)

    val outputPath = args(1)

    val conf = new SparkConf().setAppName("InvertedIdx")
    val sc = new SparkContext(conf)

    // Cleanup output dir
    val hadoopConf = sc.hadoopConfiguration
    var hdfs = FileSystem.get(hadoopConf)
    try { hdfs.delete(new Path(outputPath), true) } catch { case _: Throwable => {} }

    val lines = sc.textFile(filePath, sc.defaultParallelism * 5)

    val regex = "\\[\\[(.+?)([\\|#]|\\]\\])".r;

    var st = System.nanoTime

    var link =
      lines.map(line => {
        val lineXml = scala.xml.XML.loadString(line.toString())
        val title = (lineXml \ "title").text;
        val pagetext = (lineXml \  "revision" \ "text").text;
        val out = regex.findAllIn(lineXml.text).toArray
          .map { x => x.replaceAll("[\\[\\]]", "").split("[\\|#]") }
          .filter { _.length > 0 }.map(_.head.capitalize);

        out.map { x => (x, title) }.+:(title, "&gt")
      }).flatMap(y => y).groupByKey(sc.defaultParallelism * 5).filter(_._2.exists { _ == "&gt" })
        //
        .map(row => {
          row._2.toArray.filter(_ != "&gt").map(tp => (tp, row._1)).+:(row._1, "&gt");
        }).flatMap(y => y).groupByKey(sc.defaultParallelism * 5).map(x => (x._1, x._2.toArray.filter { _ != "&gt" }))
      link =  link.partitionBy(new HashPartitioner(sc.defaultParallelism * 5));
        
    link = link.cache();

    sc.stop
  }
}